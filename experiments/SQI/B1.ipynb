{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9f8b985",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\chand\\anaconda3\\envs\\AllPackages\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv1D, BatchNormalization, Dropout, LSTM, Dense, Multiply, Lambda, Softmax, TimeDistributed\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b953e814",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------- Load & Preprocess -----------------\n",
    "df = pd.read_csv(\"SQI.csv\")\n",
    "df = df.sort_values(['subject']).reset_index(drop=True)\n",
    "\n",
    "df.columns = df.columns.str.lower()\n",
    "\n",
    "# ----------------- Impute missing HR values -----------------\n",
    "df['hr'] = df.groupby('subject')['hr'].transform(lambda x: x.fillna(x.mean()))\n",
    "df['hr'] = df['hr'].fillna(df['hr'].mean())  # global mean fallback\n",
    "\n",
    "features = ['bvp', 'temp', 'eda', 'hr']\n",
    "target = 'sleep_stage'  # target\n",
    "\n",
    "# Standardize features (fit only on training subjects later)\n",
    "scaler = StandardScaler()\n",
    "df[features] = scaler.fit_transform(df[features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0aed44bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import mode\n",
    "\n",
    "# ----------------- Windowing -----------------\n",
    "window_size = 60\n",
    "step_size = 30\n",
    "sequence_length = 5  # number of windows per sequence for inter-window modeling\n",
    "\n",
    "def create_windows_per_subject(df, features, target, subjects_list):\n",
    "    sequences = []\n",
    "    labels = []\n",
    "    for subject in subjects_list:\n",
    "        df_sub = df[df['subject'] == subject].reset_index(drop=True)\n",
    "        X_sub, y_sub = [], []\n",
    "        # Create windows\n",
    "        for i in range(0, len(df_sub) - window_size + 1, step_size):\n",
    "            X_sub.append(df_sub[features].iloc[i:i+window_size].values)\n",
    "            # Correct way to get mode in newer SciPy versions\n",
    "            y_sub.append(mode(df_sub[target].iloc[i:i+window_size], keepdims=True).mode[0])\n",
    "        X_sub = np.array(X_sub)\n",
    "        y_sub = np.array(y_sub)\n",
    "        # Create sequences of windows\n",
    "        for i in range(0, len(X_sub) - sequence_length + 1, 1):\n",
    "            sequences.append(X_sub[i:i+sequence_length])\n",
    "            labels.append(mode(y_sub[i:i+sequence_length], keepdims=True).mode[0])\n",
    "    return np.array(sequences), np.array(labels)\n",
    "\n",
    "# ----------------- Split subjects -----------------\n",
    "test_subjects = ['S015', 'S016']\n",
    "train_subjects = [s for s in df['subject'].unique() if s not in test_subjects]\n",
    "\n",
    "X_train, y_train = create_windows_per_subject(df, features, target, train_subjects)\n",
    "X_test, y_test = create_windows_per_subject(df, features, target, test_subjects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c61176c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\chand\\anaconda3\\envs\\AllPackages\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 5, 60, 4)]        0         \n",
      "                                                                 \n",
      " time_distributed_6 (TimeDi  (None, 5, 60, 64)         832       \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_7 (TimeDi  (None, 5, 60, 64)         256       \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_8 (TimeDi  (None, 5, 60, 64)         0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_9 (TimeDi  (None, 5, 60, 32)         6176      \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_10 (TimeD  (None, 5, 60, 32)         128       \n",
      " istributed)                                                     \n",
      "                                                                 \n",
      " time_distributed_11 (TimeD  (None, 5, 60, 64)         24832     \n",
      " istributed)                                                     \n",
      "                                                                 \n",
      " time_distributed_12 (TimeD  (None, 5, 64)             65        \n",
      " istributed)                                                     \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 5, 64)             33024     \n",
      "                                                                 \n",
      " time_distributed_13 (TimeD  (None, 5)                 65        \n",
      " istributed)                                                     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 6)                 36        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 65414 (255.52 KB)\n",
      "Trainable params: 65222 (254.77 KB)\n",
      "Non-trainable params: 192 (768.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Layer\n",
    "\n",
    "# ----------------- Custom Attention Layer -----------------\n",
    "class Attention(Layer):\n",
    "    def __init__(self):\n",
    "        super(Attention, self).__init__()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(shape=(input_shape[-1], 1),\n",
    "                                 initializer='random_normal',\n",
    "                                 trainable=True)\n",
    "        self.b = self.add_weight(shape=(1,),\n",
    "                                 initializer='zeros',\n",
    "                                 trainable=True)\n",
    "        super(Attention, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # inputs: (batch, timesteps, features)\n",
    "        score = K.tanh(K.dot(inputs, self.W) + self.b)  # (batch, timesteps, 1)\n",
    "        attention_weights = K.softmax(score, axis=1)     # (batch, timesteps, 1)\n",
    "        context_vector = attention_weights * inputs     # (batch, timesteps, features)\n",
    "        context_vector = K.sum(context_vector, axis=1)  # (batch, features)\n",
    "        return context_vector\n",
    "\n",
    "# ----------------- Hierarchical Model -----------------\n",
    "num_windows = X_train.shape[1]\n",
    "window_size = X_train.shape[2]\n",
    "num_features = X_train.shape[3]\n",
    "\n",
    "input_layer = Input(shape=(num_windows, window_size, num_features))\n",
    "\n",
    "# Intra-window feature extraction using TimeDistributed\n",
    "x = TimeDistributed(Conv1D(64, 3, activation='relu', padding='same'))(input_layer)\n",
    "x = TimeDistributed(BatchNormalization())(x)\n",
    "x = TimeDistributed(Dropout(0.2))(x)\n",
    "x = TimeDistributed(Conv1D(32, 3, activation='relu', padding='same'))(x)\n",
    "x = TimeDistributed(BatchNormalization())(x)\n",
    "x = TimeDistributed(LSTM(64, return_sequences=True))(x)\n",
    "x = TimeDistributed(Attention())(x)  # shape: (batch, num_windows, 64)\n",
    "\n",
    "# Inter-window modeling\n",
    "x = LSTM(64, return_sequences=True)(x)\n",
    "x = TimeDistributed(Attention())(x)  # final context vector\n",
    "\n",
    "# Output layer: multi-class classification\n",
    "output = Dense(len(df[target].unique()), activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=input_layer, outputs=output)\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "36651ab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:From c:\\Users\\chand\\anaconda3\\envs\\AllPackages\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\chand\\anaconda3\\envs\\AllPackages\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "773/773 [==============================] - 59s 70ms/step - loss: 1.4252 - accuracy: 0.4873 - val_loss: 1.4198 - val_accuracy: 0.5437\n",
      "Epoch 2/50\n",
      "773/773 [==============================] - 47s 61ms/step - loss: 1.2565 - accuracy: 0.5367 - val_loss: 1.3343 - val_accuracy: 0.5837\n",
      "Epoch 3/50\n",
      "773/773 [==============================] - 40s 52ms/step - loss: 1.1687 - accuracy: 0.5633 - val_loss: 1.4355 - val_accuracy: 0.4782\n",
      "Epoch 4/50\n",
      "773/773 [==============================] - 40s 51ms/step - loss: 1.0668 - accuracy: 0.6057 - val_loss: 1.3055 - val_accuracy: 0.5626\n",
      "Epoch 5/50\n",
      "773/773 [==============================] - 40s 52ms/step - loss: 0.9884 - accuracy: 0.6346 - val_loss: 1.4930 - val_accuracy: 0.4869\n",
      "Epoch 6/50\n",
      "773/773 [==============================] - 45s 58ms/step - loss: 0.9336 - accuracy: 0.6525 - val_loss: 1.4507 - val_accuracy: 0.4927\n",
      "Epoch 7/50\n",
      "773/773 [==============================] - 42s 55ms/step - loss: 0.8856 - accuracy: 0.6693 - val_loss: 1.5395 - val_accuracy: 0.4418\n",
      "Epoch 8/50\n",
      "773/773 [==============================] - 42s 54ms/step - loss: 0.8304 - accuracy: 0.6935 - val_loss: 1.5139 - val_accuracy: 0.4978\n",
      "Epoch 9/50\n",
      "773/773 [==============================] - 42s 54ms/step - loss: 0.7938 - accuracy: 0.7039 - val_loss: 1.6189 - val_accuracy: 0.4789\n",
      "Epoch 10/50\n",
      "773/773 [==============================] - 43s 56ms/step - loss: 0.7586 - accuracy: 0.7170 - val_loss: 1.7793 - val_accuracy: 0.3362\n",
      "Epoch 11/50\n",
      "773/773 [==============================] - 41s 53ms/step - loss: 0.7407 - accuracy: 0.7281 - val_loss: 1.7834 - val_accuracy: 0.4432\n",
      "Epoch 12/50\n",
      "773/773 [==============================] - 41s 53ms/step - loss: 0.7127 - accuracy: 0.7364 - val_loss: 1.9498 - val_accuracy: 0.3814\n",
      "Epoch 13/50\n",
      "773/773 [==============================] - 45s 59ms/step - loss: 0.6860 - accuracy: 0.7461 - val_loss: 2.3096 - val_accuracy: 0.3741\n",
      "Epoch 14/50\n",
      "772/773 [============================>.] - ETA: 0s - loss: 0.6634 - accuracy: 0.7518Restoring model weights from the end of the best epoch: 4.\n",
      "773/773 [==============================] - 41s 53ms/step - loss: 0.6631 - accuracy: 0.7519 - val_loss: 1.9945 - val_accuracy: 0.3734\n",
      "Epoch 14: early stopping\n"
     ]
    }
   ],
   "source": [
    "# ----------------- Train -----------------\n",
    "# Use last 10% of training sequences as validation (sequentially)\n",
    "val_split = int(len(X_train)*0.9)\n",
    "X_train_final, X_val = X_train[:val_split], X_train[val_split:]\n",
    "y_train_final, y_val = y_train[:val_split], y_train[val_split:]\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, verbose=1)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_final, y_train_final,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=50,\n",
    "    batch_size=16,\n",
    "    callbacks=[early_stop],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "05e482cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72/72 [==============================] - 3s 36ms/step\n",
      "Accuracy: 0.39198606271777003\n"
     ]
    }
   ],
   "source": [
    "# ----------------- Evaluate -----------------\n",
    "y_pred = np.argmax(model.predict(X_test), axis=-1)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ddf1ef47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv1D, BatchNormalization, Dropout, LSTM, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.stats import mode\n",
    "\n",
    "# ----------------- Load & Preprocess -----------------\n",
    "df = pd.read_csv(\"SQI.csv\")\n",
    "df = df.sort_values(['subject']).reset_index(drop=True)\n",
    "\n",
    "df.columns = df.columns.str.lower()\n",
    "\n",
    "# Impute missing HR values\n",
    "df['hr'] = df.groupby('subject')['hr'].transform(lambda x: x.fillna(x.mean()))\n",
    "df['hr'] = df['hr'].fillna(df['hr'].mean())\n",
    "\n",
    "features = ['bvp', 'temp', 'eda', 'hr', 'acc_x','acc_y','acc_z']\n",
    "target = 'sleep_stage'\n",
    "\n",
    "# ----------------- Normalize Per Subject -----------------\n",
    "df_norm = pd.DataFrame()\n",
    "for subj in df['subject'].unique():\n",
    "    df_sub = df[df['subject'] == subj].copy()\n",
    "    scaler = StandardScaler()\n",
    "    df_sub[features] = scaler.fit_transform(df_sub[features])\n",
    "    df_norm = pd.concat([df_norm, df_sub], axis=0)\n",
    "\n",
    "df = df_norm.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "39eac419",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------- Windowing -----------------\n",
    "window_size = 60\n",
    "step_size = 30\n",
    "\n",
    "X_list, y_list, subj_list = [], [], []\n",
    "\n",
    "for subject in df['subject'].unique():\n",
    "    df_sub = df[df['subject'] == subject].reset_index(drop=True)\n",
    "    for i in range(0, len(df_sub) - window_size + 1, step_size):\n",
    "        X_list.append(df_sub[features].iloc[i:i+window_size].values)\n",
    "        y_list.append(mode(df_sub[target].iloc[i:i+window_size], keepdims=True).mode[0])\n",
    "        subj_list.append(subject)\n",
    "\n",
    "X = np.array(X_list)\n",
    "y = np.array(y_list)\n",
    "subjects_per_window = np.array(subj_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c2ff9da3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train windows: (13786, 60, 7), Test windows: (2304, 60, 7)\n"
     ]
    }
   ],
   "source": [
    "# ----------------- Split by Subjects -----------------\n",
    "test_subjects = ['S015', 'S016']\n",
    "train_mask = ~np.isin(subjects_per_window, test_subjects)\n",
    "test_mask = np.isin(subjects_per_window, test_subjects)\n",
    "\n",
    "X_train, y_train = X[train_mask], y[train_mask]\n",
    "X_test, y_test = X[test_mask], y[test_mask]\n",
    "\n",
    "print(f\"Train windows: {X_train.shape}, Test windows: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7488e711",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_5 (InputLayer)        [(None, 60, 7)]           0         \n",
      "                                                                 \n",
      " conv1d_6 (Conv1D)           (None, 60, 64)            1408      \n",
      "                                                                 \n",
      " batch_normalization_6 (Bat  (None, 60, 64)            256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 60, 64)            0         \n",
      "                                                                 \n",
      " lstm_5 (LSTM)               (None, 64)                33024     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 6)                 390       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 35078 (137.02 KB)\n",
      "Trainable params: 34950 (136.52 KB)\n",
      "Non-trainable params: 128 (512.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# ----------------- Simple 1D CNN + LSTM Model -----------------\n",
    "input_layer = Input(shape=(window_size, len(features)))\n",
    "\n",
    "x = Conv1D(64, kernel_size=3, activation='relu', padding='same')(input_layer)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = LSTM(64)(x)\n",
    "output = Dense(len(np.unique(y)), activation='softmax')(x)  # multi-class classification\n",
    "\n",
    "model = Model(inputs=input_layer, outputs=output)\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dceb6376",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "776/776 [==============================] - 28s 31ms/step - loss: 1.1129 - accuracy: 0.5934 - val_loss: 1.3323 - val_accuracy: 0.5642\n",
      "Epoch 2/50\n",
      "776/776 [==============================] - 37s 48ms/step - loss: 0.9607 - accuracy: 0.6392 - val_loss: 1.4076 - val_accuracy: 0.5170\n",
      "Epoch 3/50\n",
      "776/776 [==============================] - 58s 75ms/step - loss: 0.8908 - accuracy: 0.6648 - val_loss: 1.5152 - val_accuracy: 0.5446\n",
      "Epoch 4/50\n",
      "776/776 [==============================] - 49s 63ms/step - loss: 0.8436 - accuracy: 0.6845 - val_loss: 1.5733 - val_accuracy: 0.4967\n",
      "Epoch 5/50\n",
      "776/776 [==============================] - 33s 43ms/step - loss: 0.8304 - accuracy: 0.6883 - val_loss: 1.4629 - val_accuracy: 0.5323\n",
      "Epoch 6/50\n",
      "776/776 [==============================] - 28s 37ms/step - loss: 0.8173 - accuracy: 0.6980 - val_loss: 1.6412 - val_accuracy: 0.4714\n",
      "Epoch 7/50\n",
      "776/776 [==============================] - 28s 36ms/step - loss: 0.7563 - accuracy: 0.7177 - val_loss: 1.6803 - val_accuracy: 0.4902\n",
      "Epoch 8/50\n",
      "776/776 [==============================] - 25s 32ms/step - loss: 0.7510 - accuracy: 0.7210 - val_loss: 1.6000 - val_accuracy: 0.5236\n",
      "Epoch 9/50\n",
      "776/776 [==============================] - 32s 42ms/step - loss: 0.7217 - accuracy: 0.7299 - val_loss: 1.4992 - val_accuracy: 0.5279\n",
      "Epoch 10/50\n",
      "776/776 [==============================] - 24s 31ms/step - loss: 0.7094 - accuracy: 0.7345 - val_loss: 1.8016 - val_accuracy: 0.3952\n",
      "Epoch 11/50\n",
      "776/776 [==============================] - ETA: 0s - loss: 0.6857 - accuracy: 0.7444Restoring model weights from the end of the best epoch: 1.\n",
      "776/776 [==============================] - 24s 31ms/step - loss: 0.6857 - accuracy: 0.7444 - val_loss: 1.6884 - val_accuracy: 0.4822\n",
      "Epoch 11: early stopping\n"
     ]
    }
   ],
   "source": [
    "# ----------------- Train -----------------\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, verbose=1)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_split=0.1,\n",
    "    epochs=50,\n",
    "    batch_size=16,\n",
    "    callbacks=[early_stop],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "909b6923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72/72 [==============================] - 2s 13ms/step\n",
      "\n",
      "Accuracy: 0.40234375\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ----------------- Evaluate -----------------\n",
    "y_pred = np.argmax(model.predict(X_test), axis=-1)\n",
    "print(\"\\nAccuracy:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d5564049",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13786, 60, 7)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce43e0b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AllPackages",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
